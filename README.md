# DDR Scraper Agent

Agente automatizado para recolha di√°ria de an√∫ncios p√∫blicos do [Di√°rio da Rep√∫blica](https://diariodarepublica.pt/dr/home) com suporte a IA, APIs REST e notifica√ß√µes por email.

## üìã √çndice

- [Descri√ß√£o](#descri√ß√£o)
- [Funcionalidades](#funcionalidades)
- [Arquitetura](#arquitetura)
- [Instala√ß√£o](#instala√ß√£o)
- [Configura√ß√£o](#configura√ß√£o)
- [Utiliza√ß√£o](#utiliza√ß√£o)
- [APIs REST](#apis-rest)
- [Scheduler Autom√°tico](#scheduler-autom√°tico)
- [Email Notifications](#email-notifications)
- [Desenvolvimento](#desenvolvimento)

---

## üìñ Descri√ß√£o

Sistema automatizado que recolhe an√∫ncios p√∫blicos relacionados com **seguros e dispositivos m√©dicos** do Di√°rio da Rep√∫blica (S√©rie II), filtrando por c√≥digos CPV espec√≠ficos.

O sistema oferece tr√™s formas de opera√ß√£o:
1. **Agente de IA** - Utiliza Azure OpenAI para navega√ß√£o e extra√ß√£o inteligente (com fallback autom√°tico)
2. **Scraper Tradicional** - Extra√ß√£o baseada em regras e seletores CSS
3. **APIs REST** - Endpoints HTTP para integra√ß√£o com outros sistemas

---

## ‚ú® Funcionalidades

### ü§ñ Modos de Opera√ß√£o

- **IA com Fallback**: Tenta extra√ß√£o com GPT-4, reverte para scraper tradicional se falhar
- **Manual**: Usa apenas scraper tradicional (mais r√°pido e previs√≠vel)
- **H√≠brido**: Configur√°vel via vari√°vel de ambiente `USE_AI`

### üìä Gest√£o de Dados

- Extra√ß√£o de 11 campos por an√∫ncio (SUM√ÅRIO, Publica√ß√£o, Emissor, Data, CPV, etc.)
- Filtragem por 48 c√≥digos CPV da fam√≠lia de seguros (66000000-0)
- Gera√ß√£o de Excel com formata√ß√£o profissional e descri√ß√µes de CPV
- Suporte a buffer em mem√≥ria (sem ficheiros tempor√°rios nas APIs)

### üåê APIs REST

- `GET /api/contracts/yesterday` - An√∫ncios de ontem
- `POST /api/contracts/search` - Pesquisa personalizada (CPVs, data, email)
- `GET /api/cpv-codes` - Lista de c√≥digos CPV dispon√≠veis
- `GET /health` - Health check

### ‚è∞ Scheduler Autom√°tico

- Execu√ß√£o di√°ria √†s 9h (configur√°vel)
- Tentativa IA ‚Üí Fallback Manual autom√°tico
- Envio de email ap√≥s cada execu√ß√£o
- Log de execu√ß√µes (√∫ltimas 100)

### üìß Notifica√ß√µes por Email

- Templates HTML profissionais
- Anexo Excel com dados completos
- Suporte SMTP com ou sem autentica√ß√£o
- Configur√°vel via vari√°veis de ambiente

### üîß Controlo de Ficheiros

- `SAVE_FILES_TO_DISK=true` - Salva Excel em `results/`
- `SAVE_FILES_TO_DISK=false` - Apenas envia por email (sem ficheiros permanentes)
- APIs nunca salvam ficheiros (apenas buffer/base64)

---

## üèóÔ∏è Arquitetura

```
ddr-scrapper-agent/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.ts              # Configura√ß√µes globais
‚îÇ   ‚îú‚îÄ‚îÄ cpv.ts                 # C√≥digos CPV de seguros (48 c√≥digos)
‚îÇ   ‚îú‚îÄ‚îÄ types.ts               # Interfaces TypeScript
‚îÇ   ‚îú‚îÄ‚îÄ utils.ts               # Fun√ß√µes auxiliares
‚îÇ   ‚îú‚îÄ‚îÄ server.ts              # Servidor REST API
‚îÇ   ‚îú‚îÄ‚îÄ scheduler.ts           # L√≥gica de scheduling
‚îÇ   ‚îú‚îÄ‚îÄ scheduler-main.ts      # CLI do scheduler
‚îÇ   ‚îú‚îÄ‚îÄ test-ai.ts             # Teste de Azure OpenAI
‚îÇ   ‚îú‚îÄ‚îÄ test-api.ts            # Teste de APIs REST
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ openai/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.ts          # Cliente Azure OpenAI
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent.ts           # Agente de IA (navega√ß√£o + extra√ß√£o)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ scrapper/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ diario-republica-scraper.ts    # Scraper principal
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ diario-republica-main.ts       # Script standalone
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ excel/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ excel-generator.ts # Gera√ß√£o de Excel com ExcelJS
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ email/
‚îÇ       ‚îú‚îÄ‚îÄ email-service.ts   # Servi√ßo SMTP (nodemailer)
‚îÇ       ‚îú‚îÄ‚îÄ email-sender.ts    # L√≥gica de envio
‚îÇ       ‚îî‚îÄ‚îÄ test-email.ts      # Teste de email
‚îÇ
‚îú‚îÄ‚îÄ results/                   # Excel gerados (se SAVE_FILES_TO_DISK=true)
‚îî‚îÄ‚îÄ .env                       # Vari√°veis de ambiente
```

### Fluxo de Execu√ß√£o (Scheduler)

```
Scheduler (9h di√°ria)
    ‚Üì
Tentativa 1: Scraper com IA
    ‚îú‚îÄ Sucesso ‚Üí Gerar Excel
    ‚îî‚îÄ Falha ‚Üí Tentativa 2: Scraper Manual
         ‚îú‚îÄ Sucesso ‚Üí Gerar Excel
         ‚îî‚îÄ Falha ‚Üí Log erro
              ‚Üì
        Enviar Email
              ‚Üì
    Salvar em disco (se SAVE_FILES_TO_DISK=true)
```

### Navega√ß√£o Autom√°tica

O scraper executa automaticamente:

1. **Acesso ao site** ‚Üí https://diariodarepublica.pt/dr/home
2. **Sele√ß√£o S√©rie II** ‚Üí Filtro de tipo de publica√ß√£o
3. **Sele√ß√£o de data** ‚Üí Dia anterior (ontem)
4. **Click "An√∫ncios publicados"** ‚Üí Lista de resultados
5. **Itera√ß√£o p√°gina por p√°gina**:
   - Click em cada an√∫ncio
   - Extra√ß√£o de dados
   - Verifica√ß√£o de CPV
   - Recolha se CPV v√°lido
6. **Pagina√ß√£o** ‚Üí Processa todas as p√°ginas automaticamente

---

## üöÄ Instala√ß√£o

### Pr√©-requisitos

- Node.js v18+ (recomendado v22)
- npm ou yarn

### Passos

```bash
# 1. Clonar reposit√≥rio
git clone <repo-url>
cd ddr-scrapper-agent

# 2. Instalar depend√™ncias
npm install

# 3. Instalar browsers do Playwright
npx playwright install chromium

# 4. Configurar vari√°veis de ambiente
cp .env.example .env
# Editar .env com suas configura√ß√µes

# 5. Compilar TypeScript
npm run build
```

---

## ‚öôÔ∏è Configura√ß√£o

### Ficheiro `.env`

```env
# API Server
API_PORT=3000

# Azure OpenAI (opcional - se n√£o configurado, usa apenas scraper manual)
AZURE_OPENAI_ENDPOINT=https://seu-endpoint.openai.azure.com
AZURE_OPENAI_API_KEY=sua-chave-api
AZURE_OPENAI_DEPLOYMENT=gpt-4.1
AZURE_OPENAI_API_VERSION=2024-10-01-preview

# Browser
HEADLESS=true                    # false para ver o browser
BROWSER_TIMEOUT=30000

# Scheduler
CRON_SCHEDULE=0 9 * * *          # Cron expression (9h di√°ria)
USE_AI=true                      # true = IA com fallback, false = s√≥ manual
MAX_RESULTS=                     # Limitar resultados (vazio = sem limite)
SAVE_FILES_TO_DISK=true          # Salvar Excel em results/ ?

# Email SMTP
EMAIL_HOST=smtp.exemplo.com
EMAIL_PORT=25                    # 25, 465, 587
EMAIL_USER=user@exemplo.com      # Opcional para relay interno
EMAIL_PASS=                      # Opcional para relay interno
EMAIL_FROM="DDR Scraper <noreply@exemplo.com>"
EMAIL_RECIPIENTS=email1@exemplo.com,email2@exemplo.com
```

### C√≥digos CPV

Editar [`src/cpv.ts`](src/cpv.ts) para adicionar/remover c√≥digos CPV:

```typescript
export const CPV_CODES: string[] = [
  '66000000-0',   // Servi√ßos financeiros e de seguros
  '66512200-4',   // Servi√ßos de seguro de responsabilidade civil geral
  // ... adicionar mais c√≥digos
];
```

---

## üíª Utiliza√ß√£o

### 1. Execu√ß√£o Manual

```bash
# Executar scraper com IA (fallback para manual)
npm run dev

# Apenas scraper tradicional
USE_AI=false npm run dev
```

### 2. Scheduler Autom√°tico

```bash
# Iniciar scheduler (execu√ß√£o di√°ria √†s 9h)
npm run scheduler

# Executar agora (com IA)
npm run scheduler:now

# Executar agora (for√ßar manual)
npm run scheduler:manual
```

### 3. Servidor API

```bash
# Iniciar servidor REST API
npm start

# Servidor estar√° dispon√≠vel em:
# http://localhost:3000
```

### 4. Testes

```bash
# Testar conex√£o Azure OpenAI
npm run test:ai

# Testar configura√ß√£o de email
npm run test:email

# Testar APIs REST (servidor deve estar rodando)
npm run test:api
```

---

## üåê APIs REST

### Base URL
```
http://localhost:3000
```

### Endpoints

#### 1. Health Check
```http
GET /health
```

**Resposta:**
```json
{
  "status": "ok",
  "message": "API a funcionar"
}
```

---

#### 2. An√∫ncios de Ontem
```http
GET /api/contracts/yesterday?maxResults=10
```

**Query Parameters:**
- `maxResults` (opcional) - Limitar n√∫mero de resultados

**Resposta:**
```json
{
  "success": true,
  "date": "2026-01-13",
  "totalContracts": 5,
  "maxResults": 10,
  "cpvCodes": ["66512200-4", "66515100-0"],
  "report": {
    "filename": "anuncios_seguros_2026-01-13.xlsx",
    "mimeType": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    "base64": "UEsDBBQABg..."
  },
  "contracts": [
    {
      "title": "Seguro de responsabilidade civil...",
      "publicacao": "Di√°rio da Rep√∫blica n.¬∫ 10/2026...",
      "entity": "Instituto Portugu√™s do Desporto...",
      "cpv": "66512200-4",
      "contractDate": "2026-01-13",
      "contractNumber": "12345",
      "url": "https://diariodarepublica.pt/..."
    }
  ]
}
```

---

#### 3. Pesquisa Personalizada
```http
POST /api/contracts/search
Content-Type: application/json
```

**Body:**
```json
{
  "cpvCodes": ["66512200-4", "66515100-0"],
  "date": "2026-01-13",
  "sendEmail": true,
  "maxResults": 20
}
```

**Par√¢metros:**
- `cpvCodes` (opcional) - Array de CPVs para filtrar (default: todos os 48 configurados)
- `date` (opcional) - Data no formato YYYY-MM-DD (default: ontem)
- `sendEmail` (opcional) - Enviar email com Excel anexado? (default: false)
- `maxResults` (opcional) - Limitar resultados

**Valida√ß√£o:**
- CPV codes inv√°lidos retornam erro 400 com lista de c√≥digos v√°lidos
- Data deve estar no formato YYYY-MM-DD

**Resposta:**
```json
{
  "success": true,
  "emailSuccess": true,
  "emailSendTo": ["email1@exemplo.com"],
  "date": "2026-01-13",
  "totalContracts": 3,
  "maxResults": 20,
  "cpvCodes": ["66512200-4"],
  "report": {
    "filename": "contratos_search_2026-01-14T10-30-00.xlsx",
    "mimeType": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    "base64": "UEsDBBQABg..."
  },
  "contracts": [...]
}
```

---

#### 4. C√≥digos CPV Dispon√≠veis
```http
GET /api/cpv-codes
```

**Resposta:**
```json
{
  "success": true,
  "cpvCodes": {
    "33100000": "Dispositivos m√©dicos",
    "66000000-0": "Servi√ßos financeiros e de seguros",
    "66512200-4": "Servi√ßos de seguro de responsabilidade civil geral",
    ...
  }
}
```

---

### Cancelamento de Requisi√ß√µes

As APIs detectam quando o cliente cancela a requisi√ß√£o (Ctrl+C, timeout, etc.) e param automaticamente:
- Browser √© fechado imediatamente
- Recursos s√£o libertados
- Logs informativos indicam cancelamento

---

## ‚è∞ Scheduler Autom√°tico

### Configura√ß√£o

No `.env`:
```env
CRON_SCHEDULE=0 9 * * *     # Diariamente √†s 9h
USE_AI=true                  # Tentar IA primeiro
MAX_RESULTS=                 # Sem limite
SAVE_FILES_TO_DISK=true      # Salvar em results/
```

### Comportamento

1. **Execu√ß√£o di√°ria √†s 9h**
2. **Tentativa 1**: Scraper com IA
   - Se sucesso ‚Üí Continua
   - Se falha ‚Üí Tentativa 2
3. **Tentativa 2**: Scraper Manual (sempre funciona)
4. **Gerar Excel**:
   - Se `SAVE_FILES_TO_DISK=true` ‚Üí Salva em `results/`
   - Se `false` ‚Üí Apenas em mem√≥ria para email
5. **Enviar Email** (se configurado)
6. **Registar log** em `results/execution_log.json`

### Logs de Execu√ß√£o

√öltimo 100 execu√ß√µes guardadas em `results/execution_log.json`:

```json
[
  {
    "timestamp": "2026-01-14T09:00:00.000Z",
    "mode": "IA",
    "success": true,
    "anunciosCount": 12,
    "duration": "45s",
    "error": null
  }
]
```

### Comandos

```bash
# Iniciar scheduler (modo daemon)
npm run scheduler

# Executar agora (com IA)
npm run scheduler:now

# Executar agora (for√ßar manual)
npm run scheduler:manual
```

---

## üìß Email Notifications

### Configura√ß√£o SMTP

```env
EMAIL_HOST=smtp.exemplo.com
EMAIL_PORT=25                    # 25 (relay), 465 (SSL), 587 (TLS)
EMAIL_USER=user@exemplo.com      # Opcional para relay interno
EMAIL_PASS=senha                 # Opcional para relay interno
EMAIL_FROM="DDR Scraper <noreply@exemplo.com>"
EMAIL_RECIPIENTS=email1@exemplo.com,email2@exemplo.com
```

### Tipos de Servidor SMTP Suportados

1. **Com autentica√ß√£o** (Gmail, Outlook, etc.)
   ```env
   EMAIL_USER=seu@email.com
   EMAIL_PASS=sua-senha-ou-app-password
   ```

2. **Relay interno sem autentica√ß√£o** (Exchange, Postfix)
   ```env
   EMAIL_USER=
   EMAIL_PASS=
   ```

3. **Relay com user mas sem password**
   ```env
   EMAIL_USER=noreply@empresa.com
   EMAIL_PASS=
   ```

### Template de Email

- **Assunto**: üìä Relat√≥rio Di√°rio de An√∫ncios - DD/MM/YYYY
- **Formato**: HTML profissional + texto simples
- **Anexo**: Excel com todos os an√∫ncios
- **Conte√∫do**:
  - Data e hora de execu√ß√£o
  - Modo (IA ou Manual)
  - N√∫mero de an√∫ncios encontrados
  - Informa√ß√µes sobre fonte e crit√©rios

### Testes

```bash
# Verificar configura√ß√£o e enviar email de teste
npm run test:email
```

---

## üõ†Ô∏è Desenvolvimento

### Scripts Dispon√≠veis

```bash
# Desenvolvimento
npm run dev           # Executar com tsx (hot reload)
npm run build         # Compilar TypeScript ‚Üí dist/
npm start            # Executar c√≥digo compilado

# Scheduler
npm run scheduler         # Iniciar agendamento di√°rio
npm run scheduler:now     # Executar agora (IA)
npm run scheduler:manual  # Executar agora (Manual)

# Testes
npm run test:ai      # Testar Azure OpenAI
npm run test:email   # Testar SMTP
npm run test:api     # Testar APIs REST
```

### Estrutura do C√≥digo

**Scraper Principal** (`src/scrapper/diario-republica-scraper.ts`):
```typescript
class DiarioRepublicaScraper {
  async init(headless: boolean, useAI: boolean): Promise<void>
  async navigateToAnunciosPublicados(): Promise<void>
  async collectAnuncios(maxResults?: number): Promise<DiarioAnuncio[]>
  async extractWithAI(url: string): Promise<DiarioAnuncio | null>
  async extractTraditional(url: string): Promise<DiarioAnuncio | null>
}
```

**Agente de IA** (`src/openai/agent.ts`):
```typescript
class AINavigationAgent {
  async getNextNavigationAction(page, goal, state): Promise<string>
  async extractStructuredData(html, goal): Promise<any>
  isActive(): boolean
}
```

**Excel Generator** (`src/excel/excel-generator.ts`):
```typescript
class DiarioExcelGenerator {
  async generateExcel(anuncios: DiarioAnuncio[]): Promise<Buffer>
  async generateSimpleExcel(contracts): Promise<Buffer>
  async generateSummaryExcel(anuncios, fileName): Promise<string>
}
```

### Debug

```bash
# Ver browser durante execu√ß√£o
HEADLESS=false npm run dev

# Limitar resultados para testes r√°pidos
MAX_RESULTS=3 npm run dev

# Desativar IA
USE_AI=false npm run dev
```

### Adicionar Novos CPV Codes

1. Editar `src/cpv.ts`
2. Adicionar c√≥digo √† array `CPV_CODES`
3. Adicionar descri√ß√£o ao objeto `CPV_CODES_LABELS`
4. Recompilar: `npm run build`

```typescript
export const CPV_CODES: string[] = [
  '66000000-0',
  '99999999-9',  // Novo c√≥digo
];

export const CPV_CODES_LABELS: Record<string, string> = {
  '66000000-0': 'Servi√ßos financeiros e de seguros',
  '99999999-9': 'Descri√ß√£o do novo c√≥digo',
};
```

---

## üìä Campos Extra√≠dos

| Campo | Obrigat√≥rio | Descri√ß√£o |
|-------|-------------|-----------|
| `cpvPrincipal` | ‚úÖ | C√≥digo CPV (Vocabul√°rio Principal) |
| `sumario` | ‚úÖ | Resumo do an√∫ncio |
| `publicacao` | ‚úÖ | Refer√™ncia do Di√°rio da Rep√∫blica |
| `emissor` | ‚úÖ | Entidade emissora |
| `dataPublicacao` | ‚úÖ | Data de publica√ß√£o |
| `url` | ‚úÖ | URL completo do an√∫ncio |
| `numeroAnuncio` | ‚ùå | N√∫mero de refer√™ncia |
| `entidadeAdjudicante` | ‚ùå | Entidade que adjudica |
| `precoBaseSemIVA` | ‚ùå | Valor do contrato |
| `nipc` | ‚ùå | NIPC da entidade |
| `tipoContrato` | ‚ùå | Tipo de procedimento |

---

## üìù Notas Importantes

### Limita√ß√µes

- **Sem inven√ß√£o de dados**: O scraper nunca inventa informa√ß√£o, apenas extrai o que existe
- **Dependente do site**: Mudan√ßas no HTML do Di√°rio da Rep√∫blica podem quebrar o scraper
- **Rate limiting**: O site pode bloquear IPs com muitas requisi√ß√µes
- **Azure OpenAI**: Custos associados ao uso da API

### Boas Pr√°ticas

- ‚úÖ Usar `MAX_RESULTS` durante desenvolvimento/testes
- ‚úÖ Configurar `HEADLESS=false` para debug visual
- ‚úÖ Manter `SAVE_FILES_TO_DISK=true` no scheduler (hist√≥rico)
- ‚úÖ Configurar `SAVE_FILES_TO_DISK=false` nas APIs (economia de espa√ßo)
- ‚úÖ Monitorizar logs de execu√ß√£o regularmente
- ‚úÖ Backup da pasta `results/` periodicamente

### Troubleshooting

**Scraper n√£o encontra an√∫ncios:**
- Verificar se h√° an√∫ncios publicados para a data
- Confirmar que os CPV codes est√£o corretos
- Ver browser com `HEADLESS=false`

**Azure OpenAI n√£o funciona:**
- Verificar credenciais no `.env`
- Executar `npm run test:ai`
- O sistema reverte automaticamente para scraper manual

**Email n√£o enviado:**
- Executar `npm run test:email`
- Verificar configura√ß√µes SMTP
- Para Gmail, usar App Passwords

**APIs retornam erro:**
- Verificar se servidor est√° rodando (`npm start`)
- Testar health endpoint: `curl http://localhost:3000/health`

---

## üìú Licen√ßa

[Especificar licen√ßa]

---

## üë• Contribuidores

[Lista de contribuidores]

---

## üìû Suporte

Para quest√µes ou problemas, criar issue no reposit√≥rio ou contactar [email/contacto].
